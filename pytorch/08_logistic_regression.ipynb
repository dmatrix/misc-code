{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook now replaces everything we did in manually and semi-manully in the previous\n",
    "two nodebooks. We will use\n",
    "\n",
    "1. PyTorch Models for prediction\n",
    "2. Autograd for gradient computation\n",
    "3. PyTorch Loss for loss computation\n",
    "4. PyTorch Optimizer for parameter updates\n",
    "\n",
    "A PyTorch Pipeline has three steps:\n",
    "\n",
    "1. Design a model (input_size, output_size, forward_pass)\n",
    "2. Construct the loss and optimizer\n",
    "3. Training loop\n",
    " - forward pass: compute prediction and loss\n",
    " - backward pass: gradients\n",
    " - update the weights\n",
    " \n",
    " For all the above pipeline steps, we use PyTorch classes and methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Prepare data\n",
    "\n",
    "Create tensors from our numpy dataset returned from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([455, 30]) <class 'torch.Tensor'>\n",
      "y_train shape: torch.Size([455, 1]) <class 'torch.Tensor'>\n",
      "samples: 569; features: 30\n"
     ]
    }
   ],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Split traing and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale our features\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Convert our training and test data to tensors\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "# reshape our y_ data\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}', type(X_train))\n",
    "print(f'y_train shape: {y_train.shape}', type(y_train))\n",
    "print(f'samples: {n_samples}; features: {n_features}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Design the model\n",
    "\n",
    "Our model is a linear model of type `f(x) = wx + b` and we apply sigmoid function at the end to get a probability\n",
    "between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1) # 1, is the output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply sigmoid function\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "    \n",
    "# define our PyTorch model, which is callable\n",
    "model = LogisticRegression(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Construct loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is a classification, we going to use binariy cross entropy loss function\n",
    "# and the same SGD optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10,  loss: 0.120576\n",
      "epoch: 20,  loss: 0.119965\n",
      "epoch: 30,  loss: 0.119367\n",
      "epoch: 40,  loss: 0.118783\n",
      "epoch: 50,  loss: 0.118211\n",
      "epoch: 60,  loss: 0.117650\n",
      "epoch: 70,  loss: 0.117102\n",
      "epoch: 80,  loss: 0.116565\n",
      "epoch: 90,  loss: 0.116039\n",
      "epoch: 100,  loss: 0.115524\n",
      "epoch: 110,  loss: 0.115018\n",
      "epoch: 120,  loss: 0.114523\n",
      "epoch: 130,  loss: 0.114037\n",
      "epoch: 140,  loss: 0.113561\n",
      "epoch: 150,  loss: 0.113094\n",
      "accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    # Forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    \n",
    "    # Backward pass and compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # update our weights\n",
    "    optimizer.step()\n",
    "    \n",
    "     # zero out the gradients computed in the backward pass as it sums\n",
    "    # in a list property .grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # print some status and progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch: {epoch + 1},  loss: {loss.item():4f}')\n",
    "        \n",
    "# Let's evaluate the model and want to do it outside the computational graph\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
