{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook now replaces everything we did in manually and semi-manully in the previous\n",
    "two nodebooks. We will use\n",
    "\n",
    "1. PyTorch Models for prediction\n",
    "2. Autograd for gradient computation\n",
    "3. PyTorch Loss for loss computation\n",
    "4. PyTorch Optimizer for parameter updates\n",
    "\n",
    "A PyTorch Pipeline has three steps:\n",
    "\n",
    "1. Design a model (input_size, output_size, forward_pass)\n",
    "2. Construct the loss and optimizer\n",
    "3. Training loop\n",
    " - forward pass: compute prediction\n",
    " - backward pass: gradients\n",
    " - update the weights\n",
    " \n",
    " For all the above pipeline steps, we use PyTorch classes and methods \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some training data\n",
    "# 2-d tensor, where rows are number of samples and columns are number of feartures\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "# 2-d tensor, where rows are number of samples and columns are the lables \n",
    "y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "input_size = n_features\n",
    "output_size = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciction before training f(5): 2.789\n"
     ]
    }
   ],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "# define our PyTorch model, which is callable\n",
    "model = LinearRegression(input_size, output_size)\n",
    "print(f'Prediciction before training f(5): {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use PyTorch for everything... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1; w: 1.692; loss: 0.130718\n",
      "epoch: 11; w: 1.694; loss: 0.129778\n",
      "epoch: 21; w: 1.696; loss: 0.128889\n",
      "epoch: 31; w: 1.698; loss: 0.128038\n",
      "epoch: 41; w: 1.699; loss: 0.127216\n",
      "epoch: 51; w: 1.701; loss: 0.126415\n",
      "epoch: 61; w: 1.702; loss: 0.125630\n",
      "epoch: 71; w: 1.704; loss: 0.124859\n",
      "epoch: 81; w: 1.705; loss: 0.124098\n",
      "epoch: 91; w: 1.706; loss: 0.123347\n",
      "epoch: 101; w: 1.707; loss: 0.122602\n",
      "epoch: 111; w: 1.708; loss: 0.121865\n",
      "epoch: 121; w: 1.709; loss: 0.121133\n",
      "epoch: 131; w: 1.710; loss: 0.120407\n",
      "epoch: 141; w: 1.711; loss: 0.119686\n",
      "epoch: 151; w: 1.712; loss: 0.118970\n",
      "epoch: 161; w: 1.713; loss: 0.118259\n",
      "epoch: 171; w: 1.714; loss: 0.117552\n",
      "epoch: 181; w: 1.715; loss: 0.116850\n",
      "epoch: 191; w: 1.716; loss: 0.116152\n",
      "epoch: 201; w: 1.717; loss: 0.115458\n",
      "epoch: 211; w: 1.718; loss: 0.114768\n",
      "epoch: 221; w: 1.719; loss: 0.114083\n",
      "epoch: 231; w: 1.720; loss: 0.113402\n",
      "epoch: 241; w: 1.721; loss: 0.112724\n",
      "epoch: 251; w: 1.721; loss: 0.112051\n",
      "epoch: 261; w: 1.722; loss: 0.111382\n",
      "epoch: 271; w: 1.723; loss: 0.110717\n",
      "epoch: 281; w: 1.724; loss: 0.110056\n",
      "epoch: 291; w: 1.725; loss: 0.109399\n",
      "Prediciction after training f(5): 9.434\n"
     ]
    }
   ],
   "source": [
    "learing_rate = 0.001\n",
    "n_iterations = 300\n",
    "\n",
    "# define loss as a callable function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# define the optimizer, with weights and learning rate as arguments\n",
    "optimzier = torch.optim.SGD(model.parameters(), lr=learing_rate)\n",
    "\n",
    "for epoch in range(n_iterations):\n",
    "    # prediction = use torch model\n",
    "    # forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute the loss\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass in PyTroch\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    # update the weights\n",
    "    optimzier.step()\n",
    "    \n",
    "    # zero the gradients for the next pass\n",
    "    optimzier.zero_grad()\n",
    "    \n",
    "    # print some values\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch: {epoch + 1}; w: {w[0][0].item():.3f}; loss: {l:8f}')\n",
    "        \n",
    "print(f'Prediciction after training f(5): {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
