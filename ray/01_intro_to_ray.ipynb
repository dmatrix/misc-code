{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "053fc2e1-8ccc-49b5-b01d-0cbf375eda7c",
   "metadata": {},
   "source": [
    "# An Introduction to Ray: A quick start with Ray concepts and parallel primitives \n",
    "![](images/ray_header_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98cfbdd-404e-4463-a3da-ba9dc654d70b",
   "metadata": {},
   "source": [
    "## What is Ray?\n",
    "Ray in an open source universal framework for writing scalable and distrubuted applications. Orginally concieved at [Berkeley RISELab](https://rise.cs.berkeley.edu/) in 2017 by the founders of [Anyscale](https://www.anyscale.com/), Ray now has a vibrant community and a growing ecosytem of machine learning libraries that leverage Rays' simple primitives for distributed computing and programming patterns. Think of Ray as layered archicture shown below. At center is the Ray core, providing all the distributed capabilites, fault tolerance, auto-scalability, APIs in Python, Java, and C++ (currently experimental). On top of Ray Core sit native libraries and several third-party libraries for running machine learning workloads.\n",
    "\n",
    " * **Tune**: Scalable Hyperparameter Tuning. \n",
    "\n",
    " * **RLlib**: Scalable Reinforcement Learning\n",
    "\n",
    " * **RaySGD**: Distributed Training Wrappers\n",
    "\n",
    " * **Ray Serve**: Scalable and Programmable Serving\n",
    "\n",
    " * **Datasets**: Distributed Arrow on Ray (preview)\n",
    " \n",
    "Ray can run locally on a single host or on any of the cloud providers.\n",
    "\n",
    "![](images/ray_ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399cd41e-b9ee-41f7-809f-3c0e8d22ec82",
   "metadata": {},
   "source": [
    "In this quick-start tutorial, we are going to primarly focus on Ray core, the Python framework to write simple distributed applicationsâ€”on your local host (using a single core). \n",
    "Alternatively, with a couple of lines of Python decorators, you can convert your program into a distrubuted application, taking advantage of all the cores on your machine. \n",
    "Likewise, with equally simple changes to Ray's [cluster configuration](https://docs.ray.io/en/master/configure.html), you can just as easily run your application to your cloud provider of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e55a40-6558-468a-b894-05c12243acd8",
   "metadata": {},
   "source": [
    "## Why Use Ray?\n",
    "\n",
    "While numerous Python modules and examples showcase Python's various multiprocessing modules like `multiprocessing`, the module is limited and fails to meet the rigor and demands of today's machine learning (ML) workloads, deep learning, and large scale model training. Created to focus and address these demands, Ray can run the same code on more than one machine, build applications that can leverage quick access to distributed shared memory across a cluster, implement Akka model to perform actions or manipuate shared ojects, and allows an ML engineer to develop distributed applications without system expertise and using simple and intuitive Ray primitivies. Ray along with all its native libraries and a growing third-party integrated ecosystems addresses these requirements at scale and with fault-tolerance. \n",
    "\n",
    "Before we dive into how to employ these primitives, let's cover some Ray concepts about parallel programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71f97e-4173-4151-ba4f-0b2481150e65",
   "metadata": {},
   "source": [
    "## Ray's Concept of Parallel Programming: Remote Functions\n",
    "\n",
    "A single unit of execution in Ray is a _task_. A _task_ can be a Python function that can either run on a single core or it can run on multiple cores on a single host or on multiple cores on a remote cluster. What triggers what mode to run (single or parallel) is a simple Python decorator `@ray.remote` to your Python function. Rays executes _tasks_ asynchronously, meaning that your task will be put on Ray's execution \n",
    "queue as a [Python future](https://docs.python.org/3/library/asyncio-future.html). At any later point, when the task is finished, you can access this future's value using a Python `get` primitive. We will see that in a minute.\n",
    "\n",
    "To demonstrate the Rays's merits and introduce you the concept of parallelism, we will first run a compute bound task by computing the sum of all the squares between a range of rumbers, and then use the \n",
    "same task (function) to run as a Ray task on each core."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a9dc6-d85a-46ab-87c9-0263fe8b56e1",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "597a7c27-f2b0-4d90-9af1-d5dfad285d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "import logging\n",
    "import math\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43842bf-8498-482b-acfe-d2918c47b07d",
   "metadata": {},
   "source": [
    "To see the benefits of Ray tasks and non Rays tasks, let's define some functions. We'll use simple function to illustrate how easy it's to use Ray's primitives to parallelize.\n",
    "In reallity, these functions could be far more complex. For example, in Ray Tune, these functions could be parallized to do machine learning's hyperparameter tunning. For this\n",
    "short quick start, we want to demonstrate the simplicity with which you can run Ray in local mode on your host. And with a single line of code, convert to run in parallel mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c6ecdd4d-ba7f-4c7b-91c1-07d818c43552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a range of numbers, return the sum of its square root. For large number ranges this can be compute intensive, and best if they can be parallelized.\n",
    "def none_ray_task(a: int, b: int) -> int:\n",
    "    return math.sqrt(sum(x * x for x in range(a, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf77e0-a25c-4f08-8735-ea69c8d4d64a",
   "metadata": {},
   "source": [
    "Create a similar function as a Ray Task by simply using the `@remote`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ce49b827-78d6-4fa1-8375-dce617764814",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def ray_task(a: int, b: int) -> int:\n",
    "    # get the process pid on which this task is scheduled\n",
    "    print(\"pid={}; ppid={}\".format(os.getpid(), os.getppid()))\n",
    "    return math.sqrt(sum(x * x for x in range(a, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d40b8e-6fca-42fb-b649-80169583eb93",
   "metadata": {},
   "source": [
    "### Get the number of cores on this machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6048191f-ef6e-4c91-97b2-c570d2c88d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: 12\n"
     ]
    }
   ],
   "source": [
    "n_cores = multiprocessing.cpu_count()\n",
    "print(\"Number of cores: {}\".format(n_cores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f3ffee-b2e9-4616-bdde-1ec56baecae4",
   "metadata": {},
   "source": [
    "### Execute as the local task on a single core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a130ab82-c1df-4fc6-975d-94906103e6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local return valued: 18257417214.20\n",
      "Local return valued: 18257417214.20\n",
      "Local return valued: 18257417214.20\n",
      "Local return valued: 18257417214.20\n",
      "Local return valued: 18257417214.20\n",
      "Local return valued: 18257417214.20\n",
      "Local return valued: 18257417214.20\n",
      "Local return valued: 18257417214.20\n",
      "Local return valued: 18257417214.20\n",
      "Local return valued: 18257417214.20\n",
      "Local return valued: 18257417214.20\n",
      "Local return valued: 18257417214.20\n",
      "Time elapsed for non Ray task: 10.31\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for _ in range(n_cores):\n",
    "    print(\"Local return valued: {:.2f}\". format(none_ray_task(1, 10000000)))\n",
    "print(\"Time elapsed for non Ray task: {:.2f}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7a26b-4abd-466c-bfd0-5d864cb545f1",
   "metadata": {},
   "source": [
    "### Initialize the local instance of Ray with appropriate configurations. \n",
    "\n",
    "That is, logging options and number of cores to use. On a local mode this `ray.init(...)` also launches a Dashboard that can be accessed on url: http://127.0.0.1:8265."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cce0a126-8a44-4b87-a54b-cc31d472903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True,\n",
    "    logging_level=logging.ERROR,num_cpus=n_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64213edb-d5a5-4f77-a906-c77affc4f53b",
   "metadata": {},
   "source": [
    "### Execute as Remote Ray task on multiple cores\n",
    "\n",
    "To execute as a Ray task, we use the following Python syntax:\n",
    " * func.remote(args...), this will return a Future object. This will immediately return an object ref (a *future* in Python) and then create a task that will be executed on a worker process.\n",
    " * Use `ray.get` on the returned future object to return the computed value. The computed value are stored in-memory object store. \n",
    " \n",
    " Note that we did not have to explicity wait; it seemed to have done all that eagerly, without us checking if the Ray task\n",
    " was finished. The easy with which we can simply execute Python functions in parallel is easy and intuitive while being quite\n",
    " Pythonic in invocation, behavior and composibility or chaining of method calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dec8636a-99b8-44f8-93ab-f45250eea924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "\u001b[2m\u001b[36m(pid=71676)\u001b[0m pid=71676; ppid=71673\n",
      "Remote Ray task returned value: 18257417214.20\n",
      "Time elapsed for non ray task: 9.69\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for _ in range(n_cores):\n",
    "    print(\"Remote Ray task returned value: {:.2f}\".format(ray.get(ray_task.remote(1, 10000000))))\n",
    "print(\"Time elapsed for non ray task: {:.2f}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8fee5d-274c-40da-bbdf-abbf67455537",
   "metadata": {},
   "source": [
    "## Ray's Concept of Parallel Programming: Futures\n",
    "\n",
    "As noted above, Ray executes tasks asynchronously. That is, Ray will return with a future. But the actual task is scheduled to be executed on the worker.\n",
    "You can then use this future reference to fetch the computed value by the finished remote task. All the scheduling of the futures is handled by Ray's scheduler, \n",
    "and the computed value is stored in the object store as shown in the diagram.\n",
    "\n",
    " * **Head Node**: Is the main node in a Ray cluster that talks to workers. On the local host or laptop, it's the headnode\n",
    " * **Worker**: The worker process that executes your Ray Tasks\n",
    " * **Raylet**: comprises the in-memory store for computed values from the tasks and the scheduler\n",
    " * **Global Control Store**: As the name suggests,there is only a single instance that's shared among all the worker nodes.\n",
    " \n",
    " While this tutorial will not go into details about the communication or serialization of objects across the cluster and what happens under the hood, it's fitting to get a higher level what \n",
    " are the various Ray archictural components. In the future tutorials, we will go in depth into how each of these components play and interact during an execution of a complicated Ray task in a Ray cluster.\n",
    " \n",
    "![](images/ray_cluster_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a5691-234b-4a0b-abf6-835a9c61ac7d",
   "metadata": {},
   "source": [
    "Let's illustrate an aysnchronous execution of Ray tasks and how futures can be accessed:\n",
    " * Define a Ray task\n",
    " * Invoke Ray task. This will return immediately with a future reference.\n",
    " * Print the type and value of the reference\n",
    " * Now fetch the value of the task by using the `ray.get(obj_ref)`\n",
    " * Create a list of python objects, store and then use a Ray task to fetch and compute a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3e6b55cf-a9ba-4d75-b722-8d3d5804f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def func():\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a66f29-101a-481f-a9f6-8ef349fc5c2a",
   "metadata": {},
   "source": [
    "Notice the Python objec type: It a `Raylet` object reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c6d550b-e57c-4827-94c8-7b748cbce174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectRef(9d78bd90898368caffffffffffffffffffffffff0100000001000000) <class 'ray._raylet.ObjectRef'>\n"
     ]
    }
   ],
   "source": [
    "obj_ref = func.remote()\n",
    "print(obj_ref, type(obj_ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded84bfb-e08b-483a-8d2a-f47c347b2813",
   "metadata": {},
   "source": [
    "Fetch the value from the future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a8352dd0-663f-4dde-9046-c590d88e13aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the computed and finished future: 42\n"
     ]
    }
   ],
   "source": [
    "print(f\"Value of the computed and finished future: {ray.get(obj_ref)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c027494a-0b8d-4db7-93ff-265c600640bb",
   "metadata": {},
   "source": [
    "Create a list and use Ray's `ray.put` method to create a `Raylet` instance and store it. You don't have to worry where it's stored; Ray manages its references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "61980902-0bbb-43f1-a3e1-780ff8649a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectRef(ffffffffffffffffffffffffffffffffffffffff0100000003000000) <class 'ray._raylet.ObjectRef'>\n"
     ]
    }
   ],
   "source": [
    "num_list = [ 23, 42, 93 ]\n",
    "num_ref = ray.put(num_list)\n",
    "print(num_ref, type(num_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f6f00052-7d0e-475d-b3cd-edb3ca54cf1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 42, 93]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now get the value of the original list stored\n",
    "ray.get(num_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a673e945-d5db-4e2a-ac1a-44ed95bff5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def add_list (num_list):\n",
    "    return sum(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f8049334-ad3d-4ced-928b-e97e0da61d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The remote Ray task will receice an object reference, compute the sum and return the future \n",
    "calc_ref = add_list.remote(num_ref)\n",
    "# Now we can get the value of the computed future\n",
    "ray.get(calc_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385da0bb-3802-435d-adba-aebceb0011e2",
   "metadata": {},
   "source": [
    "## Ray's Concept of Parallel Programming: Remote Stateful Objects (Actors)\n",
    "\n",
    "In the above example, all the objects'computed values are stateless. That is, they are in memory, and can be evicted or removed. What if you want to keep state of a remote object.\n",
    "Ray provides a simple api to create a remote stateful object. This introduces the concept a Remote class. Remote classes in Ray are like Actors. If you are familiar with [Akka Actors](https://doc.akka.io/docs/akka/current/typed/actors.html), the idea is similar. The Actor Model provides a higher level of abstraction for writing concurrent and distributed systems. \n",
    "It alleviates the developer from having to deal with explicit locking and thread management, making it easier to write correct concurrent and parallel systems. In Ray, an actor is essentially a stateful worker (or a service). When a new actor is instantiated, a new worker is created, and methods of the actor are scheduled on that specific worker and can access and mutate the state of that worker.\n",
    "\n",
    "That state of mutation is maintained in the shared memory: Global Control Store (GCS). In other words, if other workers need access to this object, they can simply get access by referencing.\n",
    "\n",
    "How the communication and serialization of these object references across a Ray cluster is materialized is beyond the scope of this tutorial. Future advanced tutorials will expound on these nunances\n",
    "and under the hood details. For now, we'll keep it a conceptual level and share how you can use simple Ray's Pythonic APIs to create a stateful remote object, manipulate it, and share it.\n",
    "\n",
    "We going to show a typical Actor's workflow:\n",
    "\n",
    " * Define an Remote class with `@remote` decorator\n",
    " * Instanstiate an actor and inspect its type\n",
    " * Invoke Actors remote method `score` via Ray's `remote` primitive. This will return a reference object\n",
    " * Inspect its type \n",
    " * Finally, fetch the value in the via Ray's `ray.get(ref)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "299c7567-80a5-4899-b34c-5dd9c069c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "@ray.remote\n",
    "class GoalsScored:\n",
    "    def __init__ (self) -> None:\n",
    "        self._goals = 0\n",
    "\n",
    "    def score (self) -> None:\n",
    "        self._goals = randint(1, 5)\n",
    "        return self._goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e052e91-52a4-40fc-b7c6-30ff5992a355",
   "metadata": {},
   "source": [
    "Now Lets use this  class `GoalsScored` to create an actor, which returns a reference to an Actor handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1ca1d29a-b082-4fb8-9c9f-5815494c88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(GoalsScored,e86554b48b060ca201a84e7101000000) <class 'ray.actor.ActorHandle'>\n"
     ]
    }
   ],
   "source": [
    "goals = GoalsScored.remote()\n",
    "print(goals, type(goals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c3472-1a69-46c0-8075-d8a25cc80865",
   "metadata": {},
   "source": [
    "Executes Actor's remote instance method `score` and returns an object reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7fd85038-94ae-419f-b4e7-42455f8893c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectRef(02f9b42dcf9096dba0eaca7fee0f5728794e7e600100000001000000) <class 'ray._raylet.ObjectRef'>\n"
     ]
    }
   ],
   "source": [
    "total_goals_ref = goals.score.remote()\n",
    "print(total_goals_ref, type(total_goals_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "da600042-3e13-40b1-8f74-2ef4dda687d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total goals scored: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total goals scored: {ray.get(total_goals_ref)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bde4d-38da-4e40-8456-5ee838c02673",
   "metadata": {},
   "source": [
    "## Ray's Dashoard\n",
    "\n",
    "When you install ray locally, as we did in this tutorial, and run tasks on it, Ray launches a web-based Dashboard, accessible at http://127.0.0.1:8265. This gives you real-time insight into all\n",
    "the resources Ray consumes: CPUs, Memory, Logs, Actors, etc.\n",
    "\n",
    "![](images/ray_dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653bf69a-4c98-43a9-b748-1cf153cf02cb",
   "metadata": {},
   "source": [
    "## Shutdown Ray\n",
    "\n",
    "For graceful shutdown you can simply use the `ray.showdown()` API to terminate the Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b5992635-23d7-42bf-94fa-9c1063767910",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd41fc-183c-492a-a48f-5a139d4f4bf6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We covered three high-level concetps at the heart of Ray core: Tasks, Futures, and Remote Stateful objects (Actors). Using a simple Python Ray decorator, we converted a \n",
    "local Python function into a remote Ray task, and let Ray, under the hood, execute it on multiple cores. \n",
    "\n",
    "You got a peek at Ray's simple primitives to develop distributed Python. You felt the ease and Pythonic way in Ray to write distributed Python. \n",
    "Although the examples were contrived and simple, they demonstrated Ray's ease of use. \n",
    "\n",
    "And through a dashboard launched while running ray on a local host, you got a peek into Ray's real-time resource and compute usage.\n",
    "\n",
    "Stay tuned for advanced tutorials on Ray programming patterns as well as introductory and advance tutorails on Ray's native libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53900a-d2ea-4db4-9616-d2a35d4d2769",
   "metadata": {},
   "source": [
    "## Resources and References\n",
    "\n",
    "* [Ray Documentation](https://docs.ray.io/en/master/index.html#)\n",
    "* [Futures and Promises](http://dist-prog-book.com/chapter/2/futures.html)\n",
    "* [Ray Distributed Library Patterns](https://www.anyscale.com/blog/ray-distributed-library-patterns)\n",
    "* [Anyscale Academy](https://github.com/anyscale/academy)\n",
    "* [Ray Tutorial](https://github.com/ray-project/tutorial)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
