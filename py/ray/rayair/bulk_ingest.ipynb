{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a640f10-7dd3-43cc-a4b5-5d1105c098c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import random\n",
    "from ray.air import session, Checkpoint\n",
    "from ray.air import DatasetConfig\n",
    "from ray.data import Dataset\n",
    "from ray.train.torch import TorchTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f308554e-8809-407e-ac7d-411fba1e9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our dummy model function\n",
    "def model(batch):\n",
    "    return len(batch) * 0.1 * random.uniform(0,1)\n",
    "\n",
    "def train_loop():\n",
    "    # By default, bulk loading is used and returns a Dataset object.\n",
    "    data_shard: Dataset = session.get_dataset_shard(\"train\")\n",
    "    loss = 0.0\n",
    "    # Manually iterate over the data 10 times (10 epochs).\n",
    "    for epoch in range(1, 11):\n",
    "        # for each epoch iterate over batches\n",
    "        num_batches = 0\n",
    "        for batch in data_shard.iter_batches():\n",
    "            num_batches += 1\n",
    "            batch_loss = model(batch)\n",
    "            loss += batch_loss\n",
    "        loss /= num_batches * 100\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"Doing some training on epoch: {epoch} for batches: {num_batches} and loss over batch: {loss:.3f}\")\n",
    "        session.report({\"loss\": loss, \"epoch\": epoch}, \n",
    "                       checkpoint=Checkpoint.from_dict({\"loss\": loss, \"epoch\": epoch}))\n",
    "    # View the stats for performance debugging.\n",
    "    print(data_shard.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67079f56-913a-428f-ad20-c0f52a21da6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-21 12:54:49,043\tWARNING base_trainer.py:166 -- When passing `datasets` to a Trainer, it is recommended to reserve at least 20% of node CPUs for Dataset execution by setting `_max_cpu_fraction_per_node = 0.8` in the Trainer `scaling_config`. Not doing so can lead to resource contention or hangs. See https://docs.ray.io/en/master/data/key-concepts.html#example-datasets-in-tune for more info.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-21 12:54:53 (running for 00:00:04.76)<br>Memory usage on this node: 20.9/64.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/10 CPUs, 0/0 GPUs, 0.0/39.77 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/jules/ray_results/TorchTrainer_2022-07-21_12-54-49<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  _timestamp</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_fa48d_00000</td><td>TERMINATED</td><td>127.0.0.1:3727</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         2.43943</td><td style=\"text-align: right;\">0.0271053</td><td style=\"text-align: right;\">     10</td><td style=\"text-align: right;\">  1658433293</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_execute_read_task pid=3268)\u001b[0m E0721 12:54:49.180906000 6139457536 chttp2_transport.cc:1111]          Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "2022-07-21 12:54:50,207\tINFO plugin_schema_manager.py:51 -- Loading the default runtime env schemas: ['/Users/jules/git-repos/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/working_dir_schema.json', '/Users/jules/git-repos/ray/python/ray/_private/runtime_env/../../runtime_env/schemas/pip_schema.json'].\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=3727)\u001b[0m 2022-07-21 12:54:51,257\tWARNING base_trainer.py:166 -- When passing `datasets` to a Trainer, it is recommended to reserve at least 20% of node CPUs for Dataset execution by setting `_max_cpu_fraction_per_node = 0.8` in the Trainer `scaling_config`. Not doing so can lead to resource contention or hangs. See https://docs.ray.io/en/master/data/key-concepts.html#example-datasets-in-tune for more info.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m 2022-07-21 12:54:52,202\tINFO config.py:70 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m [W ProcessGroupGloo.cpp:715] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for TorchTrainer_fa48d_00000:\n",
      "  _time_this_iter_s: 0.05585002899169922\n",
      "  _timestamp: 1658433293\n",
      "  _training_iteration: 1\n",
      "  date: 2022-07-21_12-54-53\n",
      "  done: false\n",
      "  epoch: 1\n",
      "  experiment_id: f5e20a35ecfc469e8f35629aaadc69c5\n",
      "  hostname: Juless-MacBook-Pro-16\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.02155575421004692\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 3727\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.135465145111084\n",
      "  time_this_iter_s: 2.135465145111084\n",
      "  time_total_s: 2.135465145111084\n",
      "  timestamp: 1658433293\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: fa48d_00000\n",
      "  warmup_time: 0.003225088119506836\n",
      "  \n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m Doing some training on epoch: 2 for batches: 20 and loss over batch: 0.023\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m Doing some training on epoch: 4 for batches: 20 and loss over batch: 0.024\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m Doing some training on epoch: 6 for batches: 20 and loss over batch: 0.025\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m Doing some training on epoch: 8 for batches: 20 and loss over batch: 0.025\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m Doing some training on epoch: 10 for batches: 20 and loss over batch: 0.027\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m Stage 1 read->randomize_block_order: 20/20 blocks executed in 1.03s\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Remote wall time: 129.08us min, 254.42ms max, 97.56ms mean, 1.95s total\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Remote cpu time: 128.0us min, 248.93ms max, 96.62ms mean, 1.93s total\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Peak heap memory usage (MiB): 153780224000.0 min, 155123712000.0 max, 154502758400 mean\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Output num rows: 50 min, 50 max, 50 mean, 1000 total\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Output size bytes: 600 min, 600 max, 600 mean, 12000 total\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Tasks per node: 20 min, 20 max, 20 mean; 1 nodes used\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m \n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m Stage 2 split: 20/20 blocks executed in 0s\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Remote wall time: 129.08us min, 254.42ms max, 97.56ms mean, 1.95s total\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Remote cpu time: 128.0us min, 248.93ms max, 96.62ms mean, 1.93s total\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Peak heap memory usage (MiB): 153780224000.0 min, 155140096000.0 max, 154528972800 mean\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Output num rows: 50 min, 50 max, 50 mean, 1000 total\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Output size bytes: 600 min, 600 max, 600 mean, 12000 total\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Tasks per node: 20 min, 20 max, 20 mean; 1 nodes used\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m \n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m Dataset iterator time breakdown:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * In ray.wait(): 2.07ms\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * In ray.get(): 13.2ms\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * In next_batch(): 143.82us\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * In format_batch(): 13.98ms\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * In user code: 275.25us\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m * Total time: 84.21ms\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=3733)\u001b[0m \n",
      "Result for TorchTrainer_fa48d_00000:\n",
      "  _time_this_iter_s: 0.032122135162353516\n",
      "  _timestamp: 1658433293\n",
      "  _training_iteration: 10\n",
      "  date: 2022-07-21_12-54-53\n",
      "  done: true\n",
      "  epoch: 10\n",
      "  experiment_id: f5e20a35ecfc469e8f35629aaadc69c5\n",
      "  experiment_tag: '0'\n",
      "  hostname: Juless-MacBook-Pro-16\n",
      "  iterations_since_restore: 10\n",
      "  loss: 0.027105309069180756\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 3727\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.439432144165039\n",
      "  time_this_iter_s: 0.03300213813781738\n",
      "  time_total_s: 2.439432144165039\n",
      "  timestamp: 1658433293\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: fa48d_00000\n",
      "  warmup_time: 0.003225088119506836\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-21 12:54:53,937\tINFO tune.py:737 -- Total run time: 4.88 seconds (4.76 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Create our TorchTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "train_ds = ray.data.range_tensor(1000)\n",
    "trainer = TorchTrainer(train_loop,\n",
    "                       scaling_config= ScalingConfig(num_workers=1),\n",
    "                       datasets={\"train\": train_ds},\n",
    "                      )\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd2eaa2-d148-4dec-91f9-1da442629766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.024684769441396207, 'epoch': 100, '_timestamp': 1657839954, '_time_this_iter_s': 0.035117149353027344, '_training_iteration': 100, 'time_this_iter_s': 0.035944223403930664, 'should_checkpoint': True, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 100, 'trial_id': '7f364_00000', 'experiment_id': '5b6262e8afa148a6a87455dfbbdaad79', 'date': '2022-07-14_16-05-54', 'timestamp': 1657839954, 'time_total_s': 5.424535036087036, 'pid': 1526, 'hostname': 'Juless-MacBook-Pro-16', 'node_ip': '127.0.0.1', 'config': {}, 'time_since_restore': 5.424535036087036, 'timesteps_since_restore': 0, 'iterations_since_restore': 100, 'warmup_time': 0.002769947052001953, 'experiment_tag': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e91eecd5-ca8c-4d25-b4ca-7a8ffa779dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23612264564954022"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.metrics[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8777432a-eb46-4552-9dba-3152d99c921e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23612264564954022"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.checkpoint.to_dict()['loss']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
