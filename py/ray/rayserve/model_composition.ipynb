{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214ce2a5-6b11-41d9-86f4-a2b4f0d1a6d9",
   "metadata": {},
   "source": [
    "### Model Composition ServerHandle APIs\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "### Learning Objective:\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    " * compose complex models using ServeHandle APIs\n",
    " * deploy each discreate model as a seperate model deployment\n",
    " * use a single class deployment to include individual as a single model composition\n",
    " * deploy and serve this singluar model composition\n",
    "\n",
    "\n",
    "In this short tutorial we going to use HuggingFace Transformer 🤗 to accomplish three tasks:\n",
    " 1. Analyse the sentiment of a tweet: Positive or Negative\n",
    " 2. Translate it into French\n",
    " 3. Demonstrate the model composition deployment pattern using ServeHandle APIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23d1ccb-6951-4e22-aafd-48c74c4635de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TranslationPipeline, TextClassificationPipeline\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import requests\n",
    "import ray\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709b04e-cc7d-4869-8a9f-919d20a2b4cd",
   "metadata": {},
   "source": [
    "These are example 🐦 tweets, some made up, some extracted from a dog lover's twitter handle. In a real use case,\n",
    "these could come live from a Tweeter handle using [Twitter APIs](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873b8bfd-1ee3-4d9c-bea4-e66924747ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS = [\"Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\",\n",
    "          \"Sometimes. when i am bored. i will stare at nothing. and try to convince the human. that there is a ghost\",\n",
    "          \"You little dog shit, you peed and pooed on my new carpet. Bad dog!\",\n",
    "          \"I would completely believe you. Dogs and little children - very innocent and open to seeing such things\",\n",
    "          \"You've got too much time on your paws. Go check on the skittle. under the, fridge\",\n",
    "          \"You sneaky little devil, I can't live without you!!!\",\n",
    "          \"It's true what they say about dogs: they are you BEST BUDDY, no matter what!\",\n",
    "          \"This dog is way dope, just can't enough of her\",\n",
    "          \"This dog is way cool, just can't enough of her\",\n",
    "          \"Is a dog really the best pet?\",\n",
    "          \"Cats are better than dogs\",\n",
    "          \"Totally dissastified with the dog. Worst dog ever\",\n",
    "          \"Brilliant dog! Reads my moods like a book. Senses my moods and reacts. What a companinon!\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90287e-b3b9-498c-a2fc-fca7169b90fc",
   "metadata": {},
   "source": [
    "Utiliy function to fetch a tweet; these could very well be live tweets coming from Twitter API for a user or a #hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e6e502-0e23-470d-84f3-fbfdf80f4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweet_text(i):\n",
    "    text = TWEETS[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13f240-0412-4f14-943d-98a71d5d2b43",
   "metadata": {},
   "source": [
    "### Sentiment model deployment\n",
    "\n",
    "Our class deployment model to analyse the tweet using a pretrained transformer from HuggingFace 🤗.\n",
    "Note we have number of `replicas=1` but to scale it, we can increase the number of replicas, as\n",
    "we have done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50cf772-0fa6-4eaf-8288-b1cb5836f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=1)\n",
    "class SentimentTweet:\n",
    "    def __init__(self):\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        # self.model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        # self.pipeline = TextClassificationPipeline(model=self.model, tokenizer=self.tokenizer, task=\"sentiment-analysis\")\n",
    "        pass\n",
    "\n",
    "    @ray.method(num_returns=2)\n",
    "    def sentiment(self, text: str):\n",
    "        # return self.pipeline(text)[0]['label'], self.pipeline(text)[0]['score']\n",
    "        return (\"POSTIVE\", 99.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5f64-5c28-4af0-b266-4566e1f40c1a",
   "metadata": {},
   "source": [
    "### Translation model deployment\n",
    "\n",
    "Our class to translate a tweet from English --> French using a pretrained Transformer from HuggingFace 🤗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a813a1c8-c577-4028-ad09-8b4dea439e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to translate a tweet from English --> French \n",
    "# using a pretrained Transformer from HuggingFace\n",
    "@serve.deployment(num_replicas=2)\n",
    "class TranslateTweet:\n",
    "    def __init__(self):\n",
    "        #  self.tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        #  self.model = AutoModelWithLMHead.from_pretrained(\"t5-small\")\n",
    "        #  self.use_gpu = 0 if torch.cuda.is_available() else -1\n",
    "        #  self.pipeline = TranslationPipeline(self.model, self.tokenizer, task=\"translation_en_to_fr\", device=self.use_gpu)\n",
    "        pass\n",
    "\n",
    "    def translate(self, text: str):\n",
    "        return self.pipeline(text)[0]['translation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9575b2a-11c3-40ea-8522-64ed27fdcbdf",
   "metadata": {},
   "source": [
    "### Use the Model Composition pattern\n",
    "\n",
    "A composed class is deployed with both sentiment analysis and translations models' ServeHandles initialized in the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cec76c-3ef5-4741-914b-edb2c4fd2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/composed\", num_replicas=2)\n",
    "class ComposedModel:\n",
    "    def __init__(self, translate, sentiment):\n",
    "        # fetch and initialize deployment handles\n",
    "        self.translate_model = translate\n",
    "        self.sentiment_model = sentiment\n",
    "\n",
    "    async def __call__(self, http_request):\n",
    "        data = await http_request.json()\n",
    "        sentiment_ref, score_ref =  await self.sentiment_model.sentiment.remote(data)\n",
    "        print(f\"sentiment_ref:{sentiment_ref}, score_ref:{score_ref}\")\n",
    "        trans_text_ref = await self.translate_model.translate.remote(data)\n",
    "        print(f\"trans_text_ref:{trans_text_ref}\")\n",
    "        sentiment_val, score_val = ray.get([sentiment_ref, score_ref])\n",
    "        trans_text = ray.get(trans_text_ref)\n",
    "\n",
    "        return {'Sentiment': sentiment_val, 'score': score_val, 'Translated Text': trans_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dc58b-d3a4-4ee2-876d-832983ff4642",
   "metadata": {},
   "source": [
    "Start a Ray Serve instance. Note that if Ray cluster does not exist, it will create one and attach the Ray Serve\n",
    "instance to it. If one exists it'll run on that Ray cluster instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acc61315-27e0-4b1b-a4aa-ae6abd2faa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 07:50:39,769\tINFO worker.py:1481 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m.\n",
      "\u001b[2m\u001b[36m(ServeController pid=20225)\u001b[0m INFO 2022-08-09 07:50:40,764 controller 20225 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-c0b996a62ffd847f9b166635a50042959480ac91fc72c268fb72d187' on node 'c0b996a62ffd847f9b166635a50042959480ac91fc72c268fb72d187' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(ServeController pid=20225)\u001b[0m INFO 2022-08-09 07:50:41,280 controller 20225 deployment_state.py:1232 - Adding 1 replicas to deployment 'SentimentTweet'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=20225)\u001b[0m INFO 2022-08-09 07:50:41,307 controller 20225 deployment_state.py:1232 - Adding 2 replicas to deployment 'TranslateTweet'.\n",
      "\u001b[2m\u001b[36m(ServeController pid=20225)\u001b[0m INFO 2022-08-09 07:50:41,314 controller 20225 deployment_state.py:1232 - Adding 2 replicas to deployment 'ComposedModel'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=20227)\u001b[0m INFO:     Started server process [20227]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='ComposedModel')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_cls_node = SentimentTweet.bind()\n",
    "translate_cls_node = TranslateTweet.bind()\n",
    "compose_cls_node = ComposedModel.bind(sentiment_cls_node, translate_cls_node)\n",
    "\n",
    "serve.run(compose_cls_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea6006-d578-4eaa-9e95-6b2b6d4ff04a",
   "metadata": {},
   "source": [
    "### Send HTTP requests to our deployment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68cdfc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending tweet request... : Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\n",
      "Task Error. Traceback: \u001b[36mray::ServeReplica:ComposedModel.handle_request()\u001b[39m (pid=20233, ip=127.0.0.1)\n",
      "  File \"/Users/jules/git-repos/ray/python/ray/serve/_private/utils.py\", line 231, in wrap_to_ray_error\n",
      "    raise exception\n",
      "  File \"/Users/jules/git-repos/ray/python/ray/serve/_private/replica.py\", line 420, in invoke_single\n",
      "    result = await method_to_call(*args, **kwargs)\n",
      "  File \"/var/folders/zc/tmtrbwyn321fxfv_4xh_s0qm0000gn/T/ipykernel_19991/1168296825.py\", line 10, in __call__\n",
      "TypeError: cannot unpack non-iterable ray._raylet.ObjectRef object.\n"
     ]
    }
   ],
   "source": [
    "tweet = fetch_tweet_text(0)\n",
    "print(f\"Sending tweet request... : {tweet}\")\n",
    "response = requests.post(\"http://127.0.0.1:8000/composed\", json=tweet)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e72dd6-df3b-4236-bd78-d4b0b9bec98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(TWEETS)):\n",
    "    tweet = fetch_tweet_text(i)\n",
    "    print(f\"Sending tweet request... : {tweet}\")\n",
    "    response = requests.post(\"http://127.0.0.1:8000/composed\", json=tweet)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d9b3f-e349-4af6-b9c1-e4ff0a2dab9e",
   "metadata": {},
   "source": [
    "Gracefully shutdown the Ray serve instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786fcf2e-1afa-4900-bc3f-0fc8844d5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190d581-6a31-43f2-9083-94578042efdf",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Add more tweets to `TWEETS` with different sentiments.\n",
    "2. Check the score (and if you speak and read French, what you think of the translation?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4e209-4437-426e-a425-39b443df68a1",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Instead of French, use a language transformer of your choice\n",
    "2. What about Neutral tweets? Try using [vaderSentiment](https://github.com/cjhutto/vaderSentiment)\n",
    "3. Solution for 2) is [here](https://github.com/anyscale/academy/blob/main/ray-serve/05-Ray-Serve-SentimentAnalysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476075b-d538-4ba4-84fa-02688b013a4d",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "We'll further explore model composition using [Deploymant Graph APIs](https://docs.ray.io/en/latest/serve/deployment-graph.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d2797-a9ca-4f2f-bcd2-477667109fac",
   "metadata": {},
   "source": [
    "📖 [Back to Table of Contents](./ex_00_tutorial_overview.ipynb)<br>\n",
    "➡ [Next notebook](./ex_04_inference_graphs.ipynb) <br>\n",
    "⬅️ [Previous notebook](./ex_02_ray_serve_fastapi.ipynb) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f9baa705f095037670b11d47114911b546aeb3e0a8b56407edb208eea715d47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
