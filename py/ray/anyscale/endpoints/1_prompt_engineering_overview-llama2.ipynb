{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TW7yEtaWCUQL"
   },
   "source": [
    "# Overview of Prompt Engineering Techniques & Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8TD6U9JCUQO"
   },
   "source": [
    "## Part 1: Prompt Engineering Best Practices\n",
    "\n",
    "In this section, we provide an overview of the top tips and best practices for prompting LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byL1RG2UCUQO"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMKUTILJCUQP"
   },
   "source": [
    "We first load the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCsZehLGCilc",
    "outputId": "5d3b240b-c915-4a30-b999-e457dbfae9fc"
   },
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "MODEL = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "warnings.filterwarnings('ignore')\n",
    "openai.api_base = os.getenv(\"ANYSCALE_API_BASE\", os.getenv(\"OPENAI_API_BASE\"))\n",
    "openai.api_key = os.getenv(\"ANYSCALE_API_KEY\", os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F09KkfieCUQQ"
   },
   "outputs": [],
   "source": [
    "# completion function\n",
    "def get_completion(messages, model=MODEL, temperature=1, max_tokens=300):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWBwLW6kCUQR"
   },
   "source": [
    "### Be Specific and Clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfhCFzCOCUQR"
   },
   "source": [
    "Write instructions as clear and specific as possible to get the desired LLM behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUWyqyT9CUQR",
    "outputId": "dc085a07-5682-4989-c601-dcbf533e4983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, I can recommend a movie based on your interests! From the top global trending movies, I would recommend \"Spider-Man: No Way Home\". It's a superhero movie that has been gaining a lot of attention and has received positive reviews. It's an action-packed movie that promises to deliver a thrilling experience. If you're interested in watching a movie with a mix of action, adventure, and fantasy, then this might be the right choice for you.\n"
     ]
    }
   ],
   "source": [
    "global_trending_movies = [\"The Suicide Squad\", \"Bend it like Beckman\", \"Pele\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
    "\n",
    "system_message = \"\"\"\n",
    "Your task is to recommend movies to a customer.\n",
    "\n",
    "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}.\n",
    "\n",
    "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
    "\n",
    "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today\".\n",
    "\"\"\"\n",
    "\n",
    "user_request = \"\"\"\n",
    "Please recommend a movie based on my interests.\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_request\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6hb7537CUQS"
   },
   "source": [
    "The more specific the desired the behavior you want from the model, the more specific the instructions and logic should be. Below is an example where the customer provides information about interests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1sfCPwKCUQT",
    "outputId": "b470bf6b-59d1-4afa-e483-9353327bde99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Great! Based on your interest in soccer, I would recommend \"Bend it like Beckham\"! üèàüèÜ\n",
      "\n",
      "\"Bend it like Beckham\" is a heartwarming and uplifting movie about a young Indian girl who dreams of becoming a professional soccer player, despite her traditional family's disapproval. The movie follows her journey as she learns to balance her cultural heritage with her passion for the sport. With stunning visuals, catchy music, and inspiring performances, this movie is a must-watch for any soccer fan. Plus, it's a great choice for the whole family! üåü\n",
      "\n",
      "If you're interested in watching a movie that'll leave you feeling motivated and energized, \"Bend it like Beckham\" is the perfect choice for you! üí™üèºüé¨\n",
      "\n",
      "I hope you enjoy the movie! üòä\n"
     ]
    }
   ],
   "source": [
    "global_trending_movies = [\"The Suicide Squad\", \"Pele\", \"Bend it like Beckham\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
    "\n",
    "system_message = \"\"\"\n",
    "Your task is to recommends movies to a customer.\n",
    "\n",
    "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}.\n",
    "\n",
    "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
    "\n",
    "Provide a short descripition of each movie if more than one. If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
    "\"\"\"\n",
    "\n",
    "user_request = \"\"\"\n",
    "I love soccer. Please recommend a movie based on my interests.\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_request\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IZtFaB9CUQT"
   },
   "source": [
    "### Add Delimiters\n",
    "\n",
    "Adding delimiters help to better structure instructions and the overall prompt components. This is beneficial to get more reliable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "Gy8_gFBnCUQU",
    "outputId": "210c5f43-cd54-49a9-9cd9-047c5dcf5568"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python  Sure! Here's the equivalent code in Python:\n",
       "```\n",
       "strings2 = []\n",
       "strings2.append(\"one\")\n",
       "strings2.append(\"two\")\n",
       "strings2.append(\"THREE\")\n",
       "strings2.append(\"4\")\n",
       "```\n",
       "In Python, we use `append` instead of `push` to add elements to a list. Also, note that we don't need to explicitly declare the data type of `strings2` since Python is dynamically typed and can infer the data type from the context.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Convert the following code block in the #### <code> #### section to Python:\n",
    "\n",
    "####\n",
    "strings2.push(\"one\")\n",
    "strings2.push(\"two\")\n",
    "strings2.push(\"THREE\")\n",
    "strings2.push(\"4\")\n",
    "####\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "IPython.display.Markdown(\"```python\" + get_completion(message) + \"\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leL09Rl1CUQU"
   },
   "source": [
    "### Specify Output Format\n",
    "\n",
    "If the format of prompt responses are important, then this should be explicitly stated in the prompt to get desired results. In the example, we would like to export the results as a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FBIuxnKCUQU",
    "outputId": "565a80d2-9a51-41a9-ac9e-a2e0b398b31d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  {\n",
      "\"product_name\": \"Nike Air Max 270 React\",\n",
      "\"product_brand\": \"Nike\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is: given a product description, return the requested information in the section delimited by ### ###. Format the output as a JSON object.\n",
    "\n",
    "Product Description: Introducing the Nike Air Max 270 React: a comfortable and stylish sneaker that combines two of Nike's best technologies. With a sleek black design and a unique bubble sole, these shoes are perfect for everyday wear.\n",
    "\n",
    "###\n",
    "product_name: the name of the product\n",
    "product_bran: the name of the brand (if any)\n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "print(get_completion(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8XnZQ3kCUQU"
   },
   "source": [
    "### Think Step by Step\n",
    "\n",
    "To elicit reasoning in LLMs, you can prompt the model to think step-by-step. Prompting the model in this way allows it to provide the details steps before providing a final response that solves the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKQz73H8CUQV",
    "outputId": "e2f7c410-585a-411f-be58-a2bc066ea6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Let's break down the problem into steps:\n",
      "\n",
      "Step 1: Identify the odd numbers in the group.\n",
      "\n",
      "The odd numbers in the group are 5, 13, 7, and 1.\n",
      "\n",
      "Step 2: Add the odd numbers together.\n",
      "\n",
      "5 + 13 + 7 + 1 = 26\n",
      "\n",
      "Step 3: Determine whether the result is odd or even.\n",
      "\n",
      "26 is an even number.\n",
      "\n",
      "Therefore, the sum of the odd numbers in the group is 26, which is an even number.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The odd numbers in this group may add up to an even or odd number: 15, 32, 5, 13, 82, 7, 1.\n",
    "\n",
    "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response= get_completion(messages, temperature=1)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TP-pf8o441pv",
    "outputId": "29143ace-9c5b-4000-8c30-72d6cd371281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's how we can identify the prime numbers in the given group and determine whether their sum is odd or even:\n",
      "\n",
      "Step 1: Identify the prime numbers in the group:\n",
      "\n",
      "* 5 is prime.\n",
      "* 13 is prime.\n",
      "* 7 is prime.\n",
      "\n",
      "Step 2: Add the prime numbers:\n",
      "\n",
      "5 + 13 + 7 = 25\n",
      "\n",
      "Step 3: Determine whether the sum is odd or even:\n",
      "\n",
      "25 is odd.\n",
      "\n",
      "Therefore, the prime numbers in the group are 5, 13, and 7, and their sum is odd.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"From these numbers in this group identify all prime numbers: 15, 32, 5, 13, 82, 7, 1.\n",
    "\n",
    "Solve by breaking the problem into steps. First, identify the prime numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response= get_completion(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYyOpLGuCUQV"
   },
   "source": [
    "### Role Playing\n",
    "\n",
    "The example below shows how to apply role playing using a chat model like GPT-3.5 Turbo. Notice the use of system message, user message, and assistant message in the example. You can combine different messages to mimic or jump start the behavior you want or expect from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrXEZb-MCUQV",
    "outputId": "53e08436-7340-4522-baeb-2b9e915221ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! The number of stars in the Milky Way Galaxy is estimated to be between 200 and 400 billion. The exact number is difficult to determine because many stars are too distant or too faint to be detected using current telescope technology. However, ongoing surveys and missions are continually providing new data that are helping scientists refine their estimates. For example, the European Space Agency's Gaia mission, which launched in 2013, has been mapping the stars in the Milky Way Galaxy with unprecedented precision and has released several datasets that have been used to estimate the number of stars in the galaxy. The most recent estimate, based on the Gaia Data Release 2 (DR2) catalog, suggests that there are approximately 292 billion stars in the Milky Way. However, this number is likely to continue to be refined as new data becomes available.\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"\n",
    "The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
    "\"\"\"\n",
    "\n",
    "user_message_1 = \"\"\"\n",
    "Hello, who are you?\n",
    "\"\"\"\n",
    "\n",
    "ai_message_1 = \"\"\"\n",
    "Greeting! I am an AI research assistant. How can I help you today?\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Human: Can you tell me the number of stars in our Milky Way Galaxy?\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message_1\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": ai_message_1\n",
    "\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
