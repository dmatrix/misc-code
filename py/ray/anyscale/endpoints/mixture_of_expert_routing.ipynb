{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1832cea7-c09e-4095-bccb-c71957240931",
   "metadata": {},
   "source": [
    "The Router Chain in [LangChain](https://www.langchain.com/) is used with [Anyscale Endpoints](https://console.endpoints.anyscale.com/) \n",
    "to write a Mixed-expert Q & A AI system. Each \"subject\" such as Math, Physics, Astronomy, Cosmology, Football, or Computer Science has a specific LangChain bound to specific Llama 2 type of model. \n",
    "\n",
    "Anyscale Endpoints support these open-source LLM models today:\n",
    " * meta-llama/Llama-2-7b-chat-hf\n",
    " * meta-llama/Llama-2-13b-chat-hf\n",
    " * meta-llama/Llama-2-70b-chat-hf\n",
    " * codellama/CodeLlama-34b-Instruct-hf\n",
    " \n",
    "<img src=\"anyscale_endpoints.png\" height=\"35%\" width=\"%75\">\n",
    " \n",
    "You can think of these specific subject matter experts as LLMs that have been fine-tuned models with specific tasks or suitable to only answers questions related to subject area expertise.\n",
    "\n",
    "<img src=\"router_chain.png\">\n",
    "\n",
    "This particular example is an extension of Router Chain disscussed \n",
    "in the [Deeplearning.ai and LangChain course](https://learn.deeplearning.ai/langchain/lesson/4/chains) by Andrew Ng and Harrison Chase, modified and extended here to work with [Anyscale Endpoints](https://console.endpoints.anyscale.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24207468-53cf-42ed-8160-cb8aac61a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from router_prompts import (PromptInfo, MULTI_PROMPT_ROUTER_TEMPLATE)\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5aba74-8ee3-4c9a-ad34-a4191e5e72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "MODEL = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "warnings.filterwarnings('ignore')\n",
    "openai.api_base = os.getenv(\"ANYSCALE_API_BASE\", os.getenv(\"OPENAI_API_BASE\"))\n",
    "openai.api_key = os.getenv(\"ANYSCALE_API_KEY\", os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934073f2-f06e-47e7-bb36-911b6962bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your default model\n",
    "llm_default = ChatOpenAI(temperature=0.9, model_name=MODEL, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53b24b1-9d1b-4ca3-969e-d0a2c8f8d49e",
   "metadata": {},
   "source": [
    "Build destination chains, with each unique LLMChain\n",
    "bound to a specify type of Llama 2 model and\n",
    "the subject matter expert prompt to answer questions. \n",
    "For example, a physics question will be associated with a physics\n",
    "LLMChain and related trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb3567-0c88-4ff5-8c14-b5c277cafba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of chains\n",
    "destination_chains = {}\n",
    "prompt_infos = PromptInfo().prompt_infos\n",
    "for p_info in prompt_infos:\n",
    "     # name of the chain for the subject\n",
    "    name = p_info[\"name\"] \n",
    "    \n",
    "    # subject template\n",
    "    prompt_template = p_info[\"prompt_template\"] \n",
    "    \n",
    "    # fine-tuned subject-expert model\n",
    "    model_t = p_info[\"model\"] \n",
    "    llm_t = ChatOpenAI(temperature=0.9, model_name=model_t, streaming=True)\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm_t, prompt=prompt) # create the chain and its bound model\n",
    "    destination_chains[name] = chain  \n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fcd3b-b965-4966-aade-c6bd4c48d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default prompt and chain if none matches\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm_default, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6de198-b7a5-4b9c-b240-a70b75505981",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm_default, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a57f7-db11-487f-b4a8-afd3706db538",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "             \"What happens when two galaxies collide?\",\n",
    "             \"Compute a prime number larger than 1 and less than 17\",\n",
    "             \"What's the time and space complexity of merge sort compared to quicksort\",\n",
    "             \"Which is the best open source LLM model would recommend to use today\",\n",
    "             \"When was Armstice treaty signed and where and why\",\n",
    "             \"What about that actor Micheal Caine, who loves Gooner footie?\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cfdc3d-c704-4b42-8d4b-c1a46c4ca75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f38149b-b0db-450e-918a-866d78870ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_prompt(text: str):\n",
    "    return chain.run(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30801ade-fee5-4ee8-af32-ff8fd302cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "computer science: {'input': 'Which open source LLM model would you recommend using today?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "computer science: {'input': 'Write a Shell script to list files in a directory'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "history: {'input': 'When was the Armistice treaty signed and where and why'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'What is the equation for a Pythagorean triangle?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "football: {'input': 'What about that geezer actor Micheal Caine, who loves Gooner footie?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "football: {'input': 'What about that actor Micheal Caine, who loves Gooner footie?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "football: {'input': 'Tell me the best Gooner football joke from a bloke from east end'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "football: {'input': 'Tell me the best Spuds football or Tottenham club joke from a bloke from the east end'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mutli_demo= gr.Interface(fn=send_prompt, inputs=\"text\", outputs=\"text\")\n",
    "mutli_demo.launch()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9dc7c8-2811-4f02-ab90-ae2835716317",
   "metadata": {},
   "source": [
    "### Use this for debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6826083e-ae09-47f7-bcd4-2fe641ba1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    print(\"---\" * 5)\n",
    "    print(f\"question: {question}\")\n",
    "    print(chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f41aed-5add-4bb7-9a4b-673b2591a95b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
