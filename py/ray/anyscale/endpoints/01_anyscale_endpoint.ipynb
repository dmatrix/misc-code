{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5fc13fa-52f5-4948-ae75-201c145cb4cb",
   "metadata": {},
   "source": [
    "[Anyscale Endpoints](https://console.endpoints.anyscale.com/) is a managed\n",
    "service for hosting OSS LLMs. Currently, Anyscale Endpoints support these open-source LLM models:\n",
    " * meta-llama/Llama-2-7b-chat-hf\n",
    " * meta-llama/Llama-2-13b-chat-hf\n",
    " * meta-llama/Llama-2-70b-chat-hf\n",
    " * codellama/CodeLlama-34b-Instruct-hf\n",
    " \n",
    "<img src=\"anyscale_endpoints.png\" height=\"35%\" width=\"%75\">\n",
    "\n",
    "In this short example, we will use the latest CodeLlama released by [Meta AI](https://ai.meta.com/blog/code-llama-large-language-model-coding/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a5cb6c-613e-4592-b90d-d6118432c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df585f53-a79c-474b-88ce-b2815b12fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "MODEL = 'codellama/CodeLlama-34b-Instruct-hf'\n",
    "warnings.filterwarnings('ignore')\n",
    "openai.api_base = os.getenv(\"ANYSCALE_API_BASE\", os.getenv(\"OPENAI_API_BASE\"))\n",
    "openai.api_key = os.getenv(\"ANYSCALE_API_KEY\", os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b47aa1a-6f82-4cb5-bbde-e2b68c948c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = ChatOpenAI(temperature=0.9, model_name=MODEL, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f218a20f-1422-4035-95cf-7bcc2e57ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm_model, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1654b5a-ca12-4f3a-a361-4238bb1400a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_prompt(text: str):\n",
    "    return default_chain.run(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcc0e233-3053-45f5-a3af-e110b3888de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/gradio/routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/gradio/blocks.py\", line 1392, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/gradio/blocks.py\", line 1097, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/gradio/utils.py\", line 703, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/zc/tmtrbwyn321fxfv_4xh_s0qm0000gn/T/ipykernel_25389/1827600177.py\", line 2, in send_prompt\n",
      "    return default_chain.run(text)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chains/base.py\", line 451, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chains/base.py\", line 258, in __call__\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chains/base.py\", line 252, in __call__\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chains/llm.py\", line 92, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chains/llm.py\", line 102, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chat_models/base.py\", line 401, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chat_models/base.py\", line 296, in generate\n",
      "    raise e\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chat_models/base.py\", line 286, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chat_models/base.py\", line 433, in _generate_with_cache\n",
      "    return self._generate(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chat_models/openai.py\", line 389, in _generate\n",
      "    for chunk in self._stream(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chat_models/openai.py\", line 367, in _stream\n",
      "    for chunk in self.completion_with_retry(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chat_models/openai.py\", line 340, in completion_with_retry\n",
      "    return _completion_with_retry(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/tenacity/__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/tenacity/__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/concurrent/futures/_base.py\", line 437, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/tenacity/__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/langchain/chat_models/openai.py\", line 338, in _completion_with_retry\n",
      "    return self.client.create(**kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ray-core-tutorial/lib/python3.8/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: invalid model ID\n"
     ]
    }
   ],
   "source": [
    "demo= gr.Interface(fn=send_prompt, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ef302-7439-4954-a3f8-af94dd6a04df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd5d11-e56f-4833-b991-9ef87395e889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
