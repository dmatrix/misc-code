{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8053b998-8858-43a4-a742-f981962670d4",
   "metadata": {},
   "source": [
    "## Ray integration with Ray, Serve, LangChain, and HuggingFace \n",
    "\n",
    "<img src=\"ray_langchain.png\" width=\"50%\" height=\"25%\">\n",
    "\n",
    "The process involves two stages.\n",
    "\n",
    "1. Building the vector embeddings indexes from the pdf document and storing them into a Chroma database. \n",
    "For long documents, exceeding hundreds of pages, they can split and parallelized with Ray tasks. \n",
    "2. Once the Chroma vector embeddings database is built, we can read the embeddings from the disk, we load the embeddings\n",
    "from disk.\n",
    "\n",
    "<img src=\"create_index_flow.png\" width=\"70%\" height=\"25%\">\n",
    "\n",
    "Our pdf document is an AI report, which is 351 pages, small enough for one remote Ray task\n",
    "\n",
    "<img src=\"ai_report.png\" width=\"20%\" height=\"10%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd36462c-abb8-4307-9659-9eccd00ebbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from random import randint\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader \n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "\n",
    "# simple example inspired from \n",
    "# https://medium.com/geekculture/automating-pdf-interaction-with-langchain-and-chatgpt-e723337f26a6\n",
    "# and https://github.com/sophiamyang/tutorials-LangChain/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a229a94-bfa2-4e1d-bf03-843dc71b0b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"your_token\"\n",
    "vector_db_path = Path(Path.cwd(), \"vector_hf_db\").as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "750b2b50-226c-4b4f-b2e9-fefeb3cb5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/\",\n",
    "                  autoscaling_config={\n",
    "                        \"min_replicas\": 2,\n",
    "                        \"initial_replicas\": 2,\n",
    "                        \"max_replicas\": 4,\n",
    "    })\n",
    "class AnswerHuggingFacePDFQuestions():\n",
    "    def __init__(self, vector_db_path: str, hf_ai_key: str, verbose=False):\n",
    "\n",
    "        # Load the embeddings \n",
    "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_ai_key\n",
    "    \n",
    "        embeddings = HuggingFaceEmbeddings()\n",
    "        self._vectordb = self._vectordb = Chroma(persist_directory=vector_db_path, \n",
    "                                                 embedding_function=embeddings)\n",
    "        template = \"\"\"Question: {question}\n",
    "\n",
    "        Answer: \"\"\"\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "        llm_chain = LLMChain(prompt=prompt, llm=HuggingFaceHub(repo_id=\"google/flan-t5-base\",\n",
    "                                                                model_kwargs={\"temperature\":0, \"max_length\":128}))\n",
    "        self._pdf_qa_chain = llm_chain\n",
    "        self._chat_history=[]\n",
    "\n",
    "    async def __call__(self, http_request: Request) -> str:\n",
    "        json_request: str = await http_request.json()\n",
    "        prompts = []\n",
    "        for prompt in json_request:\n",
    "            text = prompt[\"text\"]\n",
    "            if isinstance(text, list):\n",
    "                prompts.extend(text)\n",
    "            else:\n",
    "                prompts.append(text)\n",
    "        result = self._pdf_qa_chain({\"question\": prompts[0], \"chat_history\": self._chat_history})\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "832e00e4-6d1d-4d61-a963-3729c7903f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=51063)\u001b[0m INFO 2023-04-21 11:11:51,810 controller 51063 http_state.py:129 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-c0e650abbe202d84d1465a53424f085d78e995352624b199b7d10e7a' on node 'c0e650abbe202d84d1465a53424f085d78e995352624b199b7d10e7a' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(ServeController pid=51063)\u001b[0m INFO 2023-04-21 11:11:52,575 controller 51063 deployment_state.py:1333 - Adding 2 replicas to deployment 'AnswerHuggingFacePDFQuestions'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=51065)\u001b[0m INFO:     Started server process [51065]\n",
      "\u001b[2m\u001b[36m(ServeReplica:AnswerHuggingFacePDFQuestions pid=51068)\u001b[0m Using embedded DuckDB with persistence: data will be stored in: /Users/jules/git-repos/misc-code/py/langchain/vector_hf_db\n",
      "\u001b[2m\u001b[36m(ServeReplica:AnswerHuggingFacePDFQuestions pid=51069)\u001b[0m Using embedded DuckDB with persistence: data will be stored in: /Users/jules/git-repos/misc-code/py/langchain/vector_hf_db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='AnswerHuggingFacePDFQuestions')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment = AnswerHuggingFacePDFQuestions.bind(vector_db_path, HF_TOKEN)\n",
    "serve.run(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e95f9-5c0f-4158-9b3c-04bcc8fb9fad",
   "metadata": {},
   "source": [
    "### Send questions to the LLM model served by Serve\n",
    "\n",
    "<img src=\"ray_serve_request_response.png\" width=\"50%\" height=\"25%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a42ad61-7a56-4b9c-9070-84c120005d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the request to the deployment\n",
    "# send the request to the deployment\n",
    "prompts = [ \"What is the total number of publications?\",\n",
    "            \"What is the percentage increase in the number of AI-related job postings?\",\n",
    "            \"Why Chinese citizens are more optimistic about AI than Americans?\",\n",
    "            # \"What are the top takeaways from this report?\",\n",
    "            # \"List benchmarks are released to evaulate AI workloads?\",\n",
    "            # \"Describe and summarize the techincal ethical issues raised in the report?\",\n",
    "            # \"How many bills containing “artificial intelligence” were passed into law?\",\n",
    "            # \"What is the percentage increase in the number of AI-related job postings?\"      \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8da65b4-d4a2-4fa0-8b6e-cb0e577f5356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the total number of publications?\n",
      "Answer: 59\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=51065)\u001b[0m INFO 2023-04-21 11:11:57,471 http_proxy 127.0.0.1 http_proxy.py:373 - POST / 200 845.8ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:AnswerHuggingFacePDFQuestions pid=51069)\u001b[0m INFO 2023-04-21 11:11:57,469 AnswerHuggingFacePDFQuestions AnswerHuggingFacePDFQuestions#fswBYg replica.py:518 - HANDLE __call__ OK 840.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the percentage increase in the number of AI-related job postings?\n",
      "Answer: a ten percent increase\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=51065)\u001b[0m INFO 2023-04-21 11:11:57,904 http_proxy 127.0.0.1 http_proxy.py:373 - POST / 200 427.0ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:AnswerHuggingFacePDFQuestions pid=51068)\u001b[0m INFO 2023-04-21 11:11:57,900 AnswerHuggingFacePDFQuestions AnswerHuggingFacePDFQuestions#GPVTIK replica.py:518 - HANDLE __call__ OK 417.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Why Chinese citizens are more optimistic about AI than Americans?\n",
      "Answer: China is a country that has a high rate of adoption of AI.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=51065)\u001b[0m INFO 2023-04-21 11:11:58,338 http_proxy 127.0.0.1 http_proxy.py:373 - POST / 200 423.4ms\n",
      "\u001b[2m\u001b[36m(ServeReplica:AnswerHuggingFacePDFQuestions pid=51069)\u001b[0m INFO 2023-04-21 11:11:58,333 AnswerHuggingFacePDFQuestions AnswerHuggingFacePDFQuestions#fswBYg replica.py:518 - HANDLE __call__ OK 417.1ms\n"
     ]
    }
   ],
   "source": [
    "sample_inputs = [{\"text\": prompt} for prompt in prompts]\n",
    "for sample_input in sample_inputs:\n",
    "    output = requests.post(\"http://localhost:8000/\", json=[sample_input]).json()\n",
    "    print(f\"Question: {output['question']}\\nAnswer: {output['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca37159c-1508-462d-a4b5-c2dabb92947f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=51063)\u001b[0m INFO 2023-04-21 11:12:06,620 controller 51063 deployment_state.py:1359 - Removing 2 replicas from deployment 'AnswerHuggingFacePDFQuestions'.\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d980889-2cde-473b-a713-358fa6564ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
